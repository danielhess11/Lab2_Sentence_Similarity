{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc3fb90",
   "metadata": {},
   "source": [
    "# Lab 6: Sentence Similarity Analysis with Lemmas\n",
    "\n",
    "Lab session by:\n",
    "* Daniel Hess\n",
    "* Pandelis Laurens Symeonidis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c7c25",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50177c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from textserver import TextServer\n",
    "\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a944edf",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2219170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Daniel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Load dataframes\n",
    "dt = pd.read_csv('./STS.input.SMTeuroparl.txt', sep='\\t', header=None)\n",
    "gold_standard_dt = pd.read_csv('./STS.gs.SMTeuroparl.txt', sep='\\t', header=None)\n",
    "\n",
    "LIMIT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99071005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_category(word: str, penn_tag: str):\n",
    "    d = {'NN': 'n', 'NNS': 'n',\n",
    "          'JJ': 'a', 'JJR': 'a', 'JJS': 'a',\n",
    "            'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', \n",
    "            'RB': 'r', 'RBR': 'r', 'RBS': 'r'} \n",
    "    \n",
    "    if penn_tag in d: \n",
    "        return word, d[penn_tag]\n",
    "    else:\n",
    "        return word, None \n",
    "\n",
    "def sentence_preparation(sentence: str):\n",
    "    # Tokenize & optional simple filtering\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # POS-tag (Penn)\n",
    "    penn_tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Map to WordNet POS and WSD\n",
    "    context = tokens  # Contex =  the tokenized sentence\n",
    "    items = []\n",
    "\n",
    "    for (w, p) in penn_tagged:\n",
    "        w2, wn_pos = convert_category(w, p)\n",
    "\n",
    "        key = None\n",
    "        if wn_pos is not None:\n",
    "        \n",
    "            syn = nltk.wsd.lesk(context, w, wn_pos)\n",
    "            if syn is None:\n",
    "                key = w\n",
    "            else:\n",
    "                key = syn.name()\n",
    "        else:\n",
    "            key = w\n",
    "\n",
    "        items.append(key)\n",
    "\n",
    "    return items\n",
    "\n",
    "def lesk_similarities(dt):\n",
    "\n",
    "    wsd1 = dt[0].apply(sentence_preparation)\n",
    "    wsd2 = dt[1].apply(sentence_preparation)\n",
    "\n",
    "    # print(wsd1.head())\n",
    "    # print(wsd2.head())\n",
    "\n",
    "    jaccard_distances_lesk = [jaccard_distance(set(s1), set(s2)) for s1, s2 in zip(wsd1, wsd2)]\n",
    "    similarities_lesk = [1 - d for d in jaccard_distances_lesk]\n",
    "\n",
    "    return similarities_lesk\n",
    "\n",
    "# Try \n",
    "# Lemmatize (lowers)\n",
    "# remove punctuation\n",
    "# regex preprocess\n",
    "# lowercasing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d93516b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_synset_from_textserver_result(x):\n",
    "    if x[-1] == 'N/A':\n",
    "        return x[0] # return token\n",
    "    id = x[-1]\n",
    "    pos, offset = id.split(\"-\")\n",
    "    pos = int(pos)\n",
    "    synset = wn.synset_from_pos_and_offset(offset, pos)\n",
    "    return synset.name()\n",
    "\n",
    "\n",
    "\n",
    "def UKB_similarities(dt):\n",
    "    ts = TextServer('pandelis2', '76Qc#iD8L', 'senses')\n",
    "    dt = dt.head(LIMIT)\n",
    "    dt = dt.applymap(lambda sentence: ts.senses(sentence))\n",
    "\n",
    "    dt_synsets = dt.applymap(lambda senses: [get_synset_from_textserver_result(x) for x in senses[0]])\n",
    "    \n",
    "    jaccard_distances = [jaccard_distance(set(s1), set(s2)) for s1, s2 in zip(dt_synsets[0], dt_synsets[1])]\n",
    "    similarities = [1 - d for d in jaccard_distances]\n",
    "\n",
    "    return similarities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b948372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_16496\\2316113642.py:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dt = dt.applymap(lambda sentence: ts.senses(sentence))\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_16496\\2316113642.py:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  dt_synsets = dt.applymap(lambda senses: [get_synset_from_textserver_result(x) for x in senses[0]])\n"
     ]
    }
   ],
   "source": [
    "similarities_lesk = lesk_similarities(dt.copy())\n",
    "similarities_UKB = UKB_similarities(dt.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfbd6e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "459\n",
      "0.39563254052490154\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(similarities_UKB))\n",
    "print(len(gold_standard_dt[0]))\n",
    "\n",
    "corr_lesk = pearsonr(similarities_lesk, gold_standard_dt[0])[0] # Calculate Pearson coefficient between similarity scores and gold standard\n",
    "corr_UKB = pearsonr(similarities_UKB, gold_standard_dt[0][:2])[0] # Calculate Pearson coefficient between similarity scores and gold standard\n",
    "\n",
    "print(corr_lesk)\n",
    "print(corr_UKB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
