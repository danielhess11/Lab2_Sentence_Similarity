{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpFFwJdnlwu0"
      },
      "source": [
        "# Lab 2: Sentence Similarity Analysis\n",
        "\n",
        "Lab session by:\n",
        "* Daniel Hess\n",
        "* Pandelis Laurens Symeonidis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adiqj_xHmim9"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BELigkBzmjO_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.metrics.distance import jaccard_distance\n",
        "from scipy.stats import pearsonr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS_cHxJ0mwMM"
      },
      "source": [
        "-there are some shortcomings in this approach, for instance if we consider the tokens the uppcercase version and lowercase version will be two diff tokens so we can check if we are getting better results with everyhting lowercase or not\n",
        "\n",
        "- what does stopword mean? Its a word with no meaning, if we add words with no meaning with same wiehghts as rest of the tokens then the metric can be poisoned, so we can try to remove the stopwords (include this in analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "jBsaaNqynry1",
        "outputId": "fd5388e5-8101-46de-b6e3-e95568e17569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original correlation: 0.4504977169318684\n",
            "Lowercase correlation: 0.4624951397591497\n",
            "Stopword removal correlation: 0.4373710526352063\n",
            "Lowercase + Stopword removal correlation: 0.4451596378377866\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "dt = pd.read_csv('./STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "gold_standard_dt = pd.read_csv('./STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "\n",
        "# Download corpus\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Tokenize and process sentences\n",
        "# dt_tokenized = dt.map(nltk.word_tokenize)\n",
        "# dt_tokenized.head()\n",
        "\n",
        "# Tokenize and option to make sentences all lower and/or removal of SW for analysis\n",
        "def preprocess_sentences(sentences, lowercase=False, remove_stopwords=False): \n",
        "    sw = set(stopwords.words('english'))\n",
        "    processed = []\n",
        "    for s in sentences:\n",
        "        tokens = nltk.word_tokenize(s)\n",
        "        if lowercase:\n",
        "            tokens = [t.lower() for t in tokens]\n",
        "        if remove_stopwords:\n",
        "            tokens = [t for t in tokens if t.lower() not in sw]\n",
        "        processed.append(tokens)\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# jaccard_distances = dt_tokenized.apply(lambda row: jaccard_distance(set(row[0]), set(row[1])), axis=1)\n",
        "# print(jaccard_distances.head())\n",
        "\n",
        "# similarity_scores = 1 - jaccard_distances\n",
        "# print(similarity_scores.shape)\n",
        "\n",
        "# Calculate jaccard distance for each sentence pair and  Pearson coefficient between similarity scores and gold standard\n",
        "def evaluate_similarity(dt, lowercase=False, remove_stopwords=False):\n",
        "    sent1 = preprocess_sentences(dt[0], lowercase, remove_stopwords)\n",
        "    sent2 = preprocess_sentences(dt[1], lowercase, remove_stopwords)\n",
        "\n",
        "    jaccard_distances = [\n",
        "        jaccard_distance(set(s1), set(s2)) for s1, s2 in zip(sent1, sent2) \n",
        "    ]\n",
        "    similarity_scores = [1 - d for d in jaccard_distances]\n",
        "\n",
        "    corr = pearsonr(similarity_scores, gold_standard_dt[0])[0] # Calculate Pearson coefficient between similarity scores and gold standard\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# comparison = pearsonr(similarity_scores, gold_standard_dt[0])\n",
        "# print(comparison[0])\n",
        "\n",
        "# Run all scenarios for comparison\n",
        "corr_original = evaluate_similarity(dt, lowercase=False, remove_stopwords=False)\n",
        "corr_lower = evaluate_similarity(dt, lowercase=True, remove_stopwords=False)\n",
        "corr_stop = evaluate_similarity(dt, lowercase=False, remove_stopwords=True)\n",
        "corr_both = evaluate_similarity(dt, lowercase=True, remove_stopwords=True)\n",
        "\n",
        "print(\"Original correlation:\", corr_original)\n",
        "print(\"Lowercase correlation:\", corr_lower)\n",
        "print(\"Stopword removal correlation:\", corr_stop)\n",
        "print(\"Lowercase + Stopword removal correlation:\", corr_both)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqYXjWjIpPX8"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results show that making the sentences lowercase improves the correlation with human similarity judgments slightly (r = 0.462 vs 0.450), which shows that case normalization helps reduce some superficial differences, making the sentence matching slightly more forgiving. On the other hand, stopword removal reduced the correlation (r = 0.437), suggesting that stopwords (\"the\", \"and\", \"to\", etc) still provide useful information for sentence matching similarity measures like Jaccard. Combining both methods yielded performance similar to the baseline (r = 0.445). Overall, this highlights how preprocessing choices can influence evaluation results, even with a simple metric."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
