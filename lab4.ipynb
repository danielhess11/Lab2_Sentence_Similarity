{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc3fb90",
   "metadata": {},
   "source": [
    "# Lab 4: \n",
    "\n",
    "Lab session by:\n",
    "* Daniel Hess\n",
    "* Pandelis Laurens Symeonidis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7c8ac",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "\n",
    "lot of pairs that are not present in the training set (why hmm is bad)\n",
    "we can use smoothing to fix it\n",
    "\n",
    "lidstone smoothing?\n",
    "passed to train_supervicse as estimator\n",
    "\n",
    "HMM = nltk.HiddenMarkovModelTagger.train(train)\n",
    "this already used LID (validate by comparing accuracy score)\n",
    "\n",
    "can save the model if needed\n",
    "\n",
    "other models we have access to in nltk:\n",
    "all in tags package (they are simply taggers we can appluy thm to any sequence tagging task)\n",
    "- TnT\n",
    "- Perceptron\n",
    "\n",
    "we just saw HMM in class (we will see CRF in the next sessions):\n",
    "we can basically build a HMM with a CRF (CRF is more complex architectyre), has feature functions\n",
    "we can change the default feature function in the CRFTagger class (for this session we DONT have to do this), the accuracy will depend on set of feature functions we are using\n",
    "\n",
    "Exercise\n",
    "\n",
    "download treebacnk\n",
    "train 4 models for different sizes\n",
    "\n",
    "evaluate all 24 models (test set is always senteces from 3001)\n",
    "plot how accuracy changes\n",
    "\n",
    "choose model and justify the answer\n",
    "just looking at learning curves isnt enough\n",
    "why? other reasons to select a model like training time. Also, inference time. Reccomendation: measure these metrics as well for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d16760",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0095cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import dill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a7fd6",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05e7119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/pandelissymeonidis/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('treebank')\n",
    "\n",
    "train_data = nltk.corpus.treebank.tagged_sents()[:3000]\n",
    "test_data = nltk.corpus.treebank.tagged_sents()[3000:]\n",
    "\n",
    "def train(train_data, model):\n",
    "    trained_model = None\n",
    "    if model == \"HMM\":\n",
    "        trained_model = nltk.HiddenMarkovModelTagger.train(train_data)\n",
    "    elif model == \"TnT\":\n",
    "        TnT = nltk.tag.tnt.TnT()\n",
    "        TnT.train(train_data)\n",
    "        trained_model = TnT\n",
    "    elif model == \"Perceptron\":\n",
    "        PER = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "        trained_model = PER.train(train_data)\n",
    "    elif model == \"CRF\":\n",
    "        CRF = nltk.tag.CRFTagger()\n",
    "        trained_model = CRF.train(train_data,'crf_tagger_model')\n",
    "    else:\n",
    "        raise Exception(\"Not valid model\")\n",
    "    filename = f'./lab4-models/{model}-{len(train_data)}.dill'\n",
    "    with open(filename, 'wb') as f:\n",
    "        dill.dump(trained_model, f)\n",
    "\n",
    "    print(f\"Successfully wrote to file {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b57ff",
   "metadata": {},
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90b8d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully wrote to file ./lab4-models/HMM-500.dill\n",
      "Successfully wrote to file ./lab4-models/HMM-1000.dill\n",
      "Successfully wrote to file ./lab4-models/HMM-1500.dill\n",
      "Successfully wrote to file ./lab4-models/HMM-2000.dill\n",
      "Successfully wrote to file ./lab4-models/HMM-2500.dill\n",
      "Successfully wrote to file ./lab4-models/TnT-500.dill\n",
      "Successfully wrote to file ./lab4-models/TnT-1000.dill\n",
      "Successfully wrote to file ./lab4-models/TnT-1500.dill\n",
      "Successfully wrote to file ./lab4-models/TnT-2000.dill\n",
      "Successfully wrote to file ./lab4-models/TnT-2500.dill\n",
      "Successfully wrote to file ./lab4-models/Perceptron-500.dill\n",
      "Successfully wrote to file ./lab4-models/Perceptron-1000.dill\n",
      "Successfully wrote to file ./lab4-models/Perceptron-1500.dill\n",
      "Successfully wrote to file ./lab4-models/Perceptron-2000.dill\n",
      "Successfully wrote to file ./lab4-models/Perceptron-2500.dill\n"
     ]
    }
   ],
   "source": [
    "models = [\"HMM\", \"TnT\", \"Perceptron\"] # TODO: Add CRF model\n",
    "sizes = [500, 1000, 1500, 2000, 2500]\n",
    "\n",
    "for model in models:\n",
    "    for size in sizes:\n",
    "        train(train_data[:size], model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9333f91",
   "metadata": {},
   "source": [
    "### Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a96ee243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./lab4-models/Perceptron-500.dill\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (trained_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     13\u001b[39m             \u001b[38;5;28mprint\u001b[39m(filename)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         accuracy = \u001b[43mtrained_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccuracy\u001b[49m(test_data)\n\u001b[32m     15\u001b[39m         results[model].append(accuracy)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'accuracy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"HMM\", \"TnT\", \"Perceptron\"] # TODO: Add CRF model\n",
    "sizes = [500, 1000, 1500, 2000, 2500]\n",
    "results = {model: [] for model in models}\n",
    "\n",
    "for model in models:\n",
    "    for size in sizes:\n",
    "        filename = f'./lab4-models/{model}-{size}.dill'\n",
    "        with open(filename, 'rb') as f:\n",
    "            trained_model = dill.load(f)\n",
    "        if (trained_model is None):\n",
    "            print(filename)\n",
    "        accuracy = trained_model.accuracy(test_data)\n",
    "        results[model].append(accuracy)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6a02a",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3f437",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5273b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "filename = f'./lab4-models/TnT-500.dill'\n",
    "with open(filename, 'rb') as f:\n",
    "    trained_model = dill.load(f)\n",
    "print(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3e892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.tag.tnt.TnT object at 0x12b643150>\n",
      "<nltk.tag.tnt.TnT object at 0x12b643150>\n",
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3ac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
