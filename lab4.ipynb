{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bc3fb90",
   "metadata": {},
   "source": [
    "# Lab 4: \n",
    "\n",
    "Lab session by:\n",
    "* Daniel Hess\n",
    "* Pandelis Laurens Symeonidis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec7c8ac",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "\n",
    "lot of pairs that are not present in the training set (why hmm is bad)\n",
    "we can use smoothing to fix it\n",
    "\n",
    "lidstone smoothing?\n",
    "passed to train_supervicse as estimator\n",
    "\n",
    "HMM = nltk.HiddenMarkovModelTagger.train(train)\n",
    "this already used LID (validate by comparing accuracy score)\n",
    "\n",
    "can save the model if needed\n",
    "\n",
    "other models we have access to in nltk:\n",
    "all in tags package (they are simply taggers we can appluy thm to any sequence tagging task)\n",
    "- TnT\n",
    "- Perceptron\n",
    "\n",
    "we just saw HMM in class (we will see CRF in the next sessions):\n",
    "we can basically build a HMM with a CRF (CRF is more complex architectyre), has feature functions\n",
    "we can change the default feature function in the CRFTagger class (for this session we DONT have to do this), the accuracy will depend on set of feature functions we are using\n",
    "\n",
    "Exercise\n",
    "\n",
    "download treebacnk\n",
    "train 4 models for different sizes\n",
    "\n",
    "evaluate all 24 models (test set is always senteces from 3001)\n",
    "plot how accuracy changes\n",
    "\n",
    "choose model and justify the answer\n",
    "just looking at learning curves isnt enough\n",
    "why? other reasons to select a model like training time. Also, inference time. Reccomendation: measure these metrics as well for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d16760",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba0095cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to C:\\Users\\Daniel\n",
      "[nltk_data]     Hess\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import dill\n",
    "import time\n",
    "import pycrfsuite \n",
    "import pathlib\n",
    "\n",
    "nltk.download('treebank')\n",
    "\n",
    "train_data = nltk.corpus.treebank.tagged_sents()[:3000]\n",
    "test_data = nltk.corpus.treebank.tagged_sents()[3000:]\n",
    "\n",
    "MODELS_DIR = pathlib.Path(\"./lab4-models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a7fd6",
   "metadata": {},
   "source": [
    "## Train and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e7119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LID(fd, bins):\n",
    "    \"\"\"Lidstone estimator (γ=0.1).\"\"\"\n",
    "    return nltk.probability.LidstoneProbDist(fd, 0.1, bins)\n",
    "\n",
    "def train(train_data, model):\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    \n",
    "    trained_model = None\n",
    "    if model == \"HMM\":\n",
    "        trained_model = nltk.HiddenMarkovModelTagger.train(train_data, estimator=LID)\n",
    "\n",
    "        save_path = MODELS_DIR / f\"HMM-{len(train_data)}.dill\"\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            dill.dump(trained_model, f)\n",
    "\n",
    "    elif model == \"TnT\":\n",
    "        TnT = nltk.tag.tnt.TnT()\n",
    "        TnT.train(train_data)\n",
    "        trained_model = TnT\n",
    "\n",
    "        save_path = MODELS_DIR / f\"TnT-{len(train_data)}.dill\"\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            dill.dump(trained_model, f)\n",
    "\n",
    "    elif model == \"Perceptron\":\n",
    "        PER = nltk.tag.perceptron.PerceptronTagger(load=False)\n",
    "        PER.train(train_data)\n",
    "        trained_model = PER\n",
    "\n",
    "        save_path = MODELS_DIR / f\"Perceptron-{len(train_data)}.dill\"\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            dill.dump(trained_model, f)\n",
    "\n",
    "    elif model == \"CRF\":\n",
    "        CRF = nltk.tag.CRFTagger()\n",
    "        save_path = MODELS_DIR / f\"CRF-{len(train_data)}.crfsuite\"\n",
    "        CRF.train(train_data, str(save_path))\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Not valid model\")\n",
    "\n",
    "    train_seconds = time.perf_counter() - t0\n",
    "    \n",
    "    print(f\"[✓] Trained {model} on {len(train_data)} sents in {train_seconds:.2f}s → {save_path}\")\n",
    "    \n",
    "    return train_seconds\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b57ff",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49681374",
   "metadata": {},
   "source": [
    "### Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✓] Trained HMM on 500 sents in 0.18s → lab4-models\\HMM-500.dill\n",
      "[✓] Trained HMM on 1000 sents in 0.34s → lab4-models\\HMM-1000.dill\n",
      "[✓] Trained HMM on 1500 sents in 0.51s → lab4-models\\HMM-1500.dill\n",
      "[✓] Trained HMM on 2000 sents in 0.69s → lab4-models\\HMM-2000.dill\n",
      "[✓] Trained HMM on 2500 sents in 0.86s → lab4-models\\HMM-2500.dill\n",
      "[✓] Trained HMM on 3000 sents in 1.01s → lab4-models\\HMM-3000.dill\n",
      "[✓] Trained HMM-LID on 500 sents in 0.18s → lab4-models\\HMM-LID-500.dill\n",
      "[✓] Trained HMM-LID on 1000 sents in 0.34s → lab4-models\\HMM-LID-1000.dill\n",
      "[✓] Trained HMM-LID on 1500 sents in 0.51s → lab4-models\\HMM-LID-1500.dill\n",
      "[✓] Trained HMM-LID on 2000 sents in 0.68s → lab4-models\\HMM-LID-2000.dill\n",
      "[✓] Trained HMM-LID on 2500 sents in 0.86s → lab4-models\\HMM-LID-2500.dill\n",
      "[✓] Trained HMM-LID on 3000 sents in 1.02s → lab4-models\\HMM-LID-3000.dill\n",
      "[✓] Trained TnT on 500 sents in 0.17s → lab4-models\\TnT-500.dill\n",
      "[✓] Trained TnT on 1000 sents in 0.23s → lab4-models\\TnT-1000.dill\n",
      "[✓] Trained TnT on 1500 sents in 0.37s → lab4-models\\TnT-1500.dill\n",
      "[✓] Trained TnT on 2000 sents in 0.48s → lab4-models\\TnT-2000.dill\n",
      "[✓] Trained TnT on 2500 sents in 0.54s → lab4-models\\TnT-2500.dill\n",
      "[✓] Trained TnT on 3000 sents in 0.69s → lab4-models\\TnT-3000.dill\n",
      "[✓] Trained Perceptron on 500 sents in 1.24s → lab4-models\\Perceptron-500.dill\n",
      "[✓] Trained Perceptron on 1000 sents in 2.34s → lab4-models\\Perceptron-1000.dill\n",
      "[✓] Trained Perceptron on 1500 sents in 3.46s → lab4-models\\Perceptron-1500.dill\n",
      "[✓] Trained Perceptron on 2000 sents in 4.59s → lab4-models\\Perceptron-2000.dill\n",
      "[✓] Trained Perceptron on 2500 sents in 5.60s → lab4-models\\Perceptron-2500.dill\n",
      "[✓] Trained Perceptron on 3000 sents in 6.42s → lab4-models\\Perceptron-3000.dill\n",
      "[✓] Trained CRF on 500 sents in 1.10s → lab4-models\\CRF-500.crfsuite\n",
      "[✓] Trained CRF on 1000 sents in 2.91s → lab4-models\\CRF-1000.crfsuite\n",
      "[✓] Trained CRF on 1500 sents in 5.03s → lab4-models\\CRF-1500.crfsuite\n",
      "[✓] Trained CRF on 2000 sents in 8.13s → lab4-models\\CRF-2000.crfsuite\n",
      "[✓] Trained CRF on 2500 sents in 11.46s → lab4-models\\CRF-2500.crfsuite\n",
      "[✓] Trained CRF on 3000 sents in 13.95s → lab4-models\\CRF-3000.crfsuite\n"
     ]
    }
   ],
   "source": [
    "models = [\"HMM\", \"TnT\", \"Perceptron\", \"CRF\"] \n",
    "sizes = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "\n",
    "training_times = {model: [] for model in models}\n",
    "\n",
    "for model in models:\n",
    "    for size in sizes:\n",
    "        t = train(train_data[:size], model)\n",
    "        training_times[model].append(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9333f91",
   "metadata": {},
   "source": [
    "### Test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ee243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model = HMM | size = 500 | accuracy = 0.793654219728038\n",
      "model = HMM | size = 1000 | accuracy = 0.8347938700625944\n",
      "model = HMM | size = 1500 | accuracy = 0.8594431254047054\n",
      "model = HMM | size = 2000 | accuracy = 0.8762357004101015\n",
      "model = HMM | size = 2500 | accuracy = 0.8864666522771423\n",
      "model = HMM | size = 3000 | accuracy = 0.8984243470753291\n",
      "model = HMM-LID | size = 500 | accuracy = 0.793654219728038\n",
      "model = HMM-LID | size = 1000 | accuracy = 0.8347938700625944\n",
      "model = HMM-LID | size = 1500 | accuracy = 0.8594431254047054\n",
      "model = HMM-LID | size = 2000 | accuracy = 0.8762357004101015\n",
      "model = HMM-LID | size = 2500 | accuracy = 0.8864666522771423\n",
      "model = HMM-LID | size = 3000 | accuracy = 0.8984243470753291\n",
      "model = TnT | size = 500 | accuracy = 0.7472911720267645\n",
      "model = TnT | size = 1000 | accuracy = 0.7962875026980358\n",
      "model = TnT | size = 1500 | accuracy = 0.827498381178502\n",
      "model = TnT | size = 2000 | accuracy = 0.8484783077919275\n",
      "model = TnT | size = 2500 | accuracy = 0.8622922512410964\n",
      "model = TnT | size = 3000 | accuracy = 0.875545003237643\n",
      "model = Perceptron | size = 500 | accuracy = 0.9133606734297431\n",
      "model = Perceptron | size = 1000 | accuracy = 0.9334340600043168\n",
      "model = Perceptron | size = 1500 | accuracy = 0.9422404489531621\n",
      "model = Perceptron | size = 2000 | accuracy = 0.9491905892510253\n",
      "model = Perceptron | size = 2500 | accuracy = 0.9524282322469242\n",
      "model = Perceptron | size = 3000 | accuracy = 0.9589035182387222\n",
      "model = CRF | size = 500 | accuracy = 0.9095618389812217\n",
      "model = CRF | size = 1000 | accuracy = 0.9249298510684222\n",
      "model = CRF | size = 1500 | accuracy = 0.9330023742715303\n",
      "model = CRF | size = 2000 | accuracy = 0.9391754802503777\n",
      "model = CRF | size = 2500 | accuracy = 0.9430174832721778\n",
      "model = CRF | size = 3000 | accuracy = 0.9474638463198791\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"HMM\", \"TnT\", \"Perceptron\", \"CRF\"] \n",
    "sizes = [500, 1000, 1500, 2000, 2500, 3000]\n",
    "results = {model: [] for model in models}\n",
    "\n",
    "for model in models:\n",
    "    for size in sizes:\n",
    "        \n",
    "        if model == \"CRF\":\n",
    "            save_path = MODELS_DIR / f\"{model}-{size}.crfsuite\"\n",
    "            trained_model = nltk.tag.CRFTagger()\n",
    "            trained_model.set_model_file(str(save_path))\n",
    "        else:\n",
    "            filename = MODELS_DIR / f\"{model}-{size}.dill\"\n",
    "            with open(filename, 'rb') as f:\n",
    "                trained_model = dill.load(f)\n",
    "\n",
    "        if (trained_model is None):\n",
    "            print(filename)\n",
    "\n",
    "        accuracy = trained_model.accuracy(test_data)\n",
    "        print(f\"model = {model} | size = {size} | accuracy = {accuracy}\")\n",
    "        results[model].append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6a02a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for m in models:  # [\"HMM\",\"TnT\",\"Perceptron\",\"CRF\"]\n",
    "    plt.plot(sizes, results[m], marker=\"o\", label=m)\n",
    "plt.xlabel(\"Train sentences\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Train Size (Treebank)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96343f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "for m in models:\n",
    "    plt.plot(sizes, training_times[m], marker=\"o\", label=m)\n",
    "plt.xlabel(\"Train sentences\")\n",
    "plt.ylabel(\"Training time (s)\")\n",
    "plt.title(\"Training Time vs Train Size\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
